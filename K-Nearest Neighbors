'''
K-Nearest Neighbors 
https://github.com/SuvroBaner/Python-for-Data-Science-and-Machine-Learning-Bootcamp/blob/master/12.%20K-Nearest-Neighbors/K%20Nearest%20Neighbors%20Project.ipynb

The error_rate formula came directly from the lecture. The logic makes sense, but doubt one has to memorize it.
'''

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
%matplotlib inline

df=pd.read_csv('data\knn')
print(df.head())

#Use seaborn on the dataframe to create a pairplot with the hue indicated by the TARGET CLASS column.
sns.pairplot(df, hue='TARGET CLASS')

#Create a StandardScaler() object called scaler.
scaler=StandardScaler()

#Fit scaler to the features.
scaler.fit(df.drop('TARGET CLASS', axis=1) )

#Use the .transform() method to transform the features to a scaled version.
scaled_features=scaler.transform(df.drop('TARGET CLASS', axis=1) )
    
#Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.
df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])
df_feat.head()
                                
#Use train_test_split to split your data into a training set and a testing set.
X=df_feat
y=df['TARGET CLASS']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

#Import KNeighborsClassifier from scikit learn.
from sklearn.neighbors import KNeighborsClassifier

#Create a KNN model instance with n_neighbors=1
knn = KNeighborsClassifier(n_neighbors=1)

#Fit this KNN model to the training data.
knn.fit(X_train, y_train)

#Use the predict method to predict values using your KNN model and X_test.
pred=knn.predict(X_test)

#Create a confusion matrix and classification report.
from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_test, pred) )

confusion_matrix(y_test, pred)
print(confusion_matrix)

#Create a for loop that trains various KNN models with different k values, then keep track of the error_rate for each of these models with a list. 
error_rate=[]

#Now create the following plot using the information from your for loop.
for i in range(1, 40):
    knn=KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred_i=knn.predict(X_test)
    error_rate.append(np.mean(pred_i !=y_test))

#Retrain your model with the best K value (up to you to decide what you want) and re-do the classification report and the confusion matrix.
plt.figure(figsize=(10,6))
plt.plot(range(1,40), error_rate, color='blue', marker='o', linestyle='dashed')
plt.title('Error Rate vs K Value')
plt.xlabel('K')
plt.ylabel('Y')
print(error_rate)
