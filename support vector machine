'''
Support Vector Machine:
Notes and example from lecture:
'''

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
%matplotlib inline

cancer=load_breast_cancer()
cancer.keys()
df_feat=pd.DataFrame(cancer['data'], columns=cancer['feature_names'])

X=df_feat
y=cancer['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

from sklearn.svm import SVC
model=SVC() 
model.fit(X_train, y_train) #A large C value gives low bias and high variance. # High gamma means small variance
predictions=model.predict(X_test)
print(confusion_matrix(y_test, predictions))
print('\n')
print(classification_report(y_test, predictions))

param_grid={'C':[.1, 1, 10, 100, 100], 'gamma':[1, .1, 0.01, .001, .0001]}
grid=GridSearchCV(SVC(), param_grid, verbose=3)
grid.fit(X_train, y_train)
grid.best_params_
grid.best_estimator_
grid_predictions=grid.predict(X_test)
print(confusion_matrix(y_test, grid_predictions))
print('\n')
print(classification_report(y_test, grid_predictions))



'''
Support Vector Machine:
https://github.com/SuvroBaner/Python-for-Data-Science-and-Machine-Learning-Bootcamp/blob/master/14.%20Support-Vector-Machines/Support%20Vector%20Machines%20Project%20.ipynb
'''
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
%matplotlib inline

iris=sns.load_dataset('iris')
iris_head=iris.head()
print(iris_head)

#create a pairplot of the dataset. Which flower species seems to be the most separable?
sns.pairplot(iris, hue='species')
print("The blue class is the most separable from the other flowers")

#create a kde plot of sepal_length versus width for setosa:
#iris['setosa'].plot.kde()
septosa=iris[iris['species']=='setosa']
sns.kdeplot(septosa['sepal_width'], septosa['sepal_length'])
print(septosa)

#Split data into a training set anda  testing set:
X=iris.drop('species', axis=1)
y=iris['species']
#df_feat=pd.DataFrame(iris['data'], columns=iris['feature_names'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

#Call the SCV() model from sklearn and fit the model to the training data:
from sklearn.svm import SVC
model=SVC() 
model.fit(X_train, y_train) #A large C value gives low bias and high variance. # High gamma means small variance

#Get predictions from the model and create a confusion maxtrix and a classification report:
predictions=model.predict(X_test)
print(confusion_matrix(y_test, predictions))
print('\n')
print(classification_report(y_test, predictions))

#Create a param_grid and fill out some parameters for C and gamma:
#Create a GridSearchCV object and fit it to the training data:
param_grid={'C':[.1, 1, 10, 100, 100], 'gamma':[1, .1, 0.01, .001, .0001]}
grid=GridSearchCV(SVC(), param_grid, verbose=3)
grid.fit(X_train, y_train)
grid.best_params_
grid.best_estimator_
grid_predictions=grid.predict(X_test)
print(confusion_matrix(y_test, grid_predictions))
print('\n')
print(classification_report(y_test, grid_predictions))





